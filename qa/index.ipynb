{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install opencv-python tf-models-official \"tensorflow-text==2.9.*\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 数据量中间计算量过大，将训练数据转换成TFRecord。省去中间重复计算（tokenid,typeid,mkid）的时间。\n",
    "# !git clone https://github.com/tensorflow/models.git\n",
    "# !python3 models/official/nlp/data/create_finetuning_data.py \\\n",
    "#  --squad_data_file=/content/squad/train-v2.0.json \\\n",
    "#  --vocab_file=gs://cloud-tpu-checkpoints/bert/v3/uncased_L-12_H-768_A-12/vocab.txt \\\n",
    "#  --train_data_output_path=/content/squad/train_v2.0.tf_record \\\n",
    "#  --meta_data_file_path=/content/squad/squad_v2.0_meta_data \\\n",
    "#  --fine_tuning_task_type=squad \\\n",
    "#  --max_seq_length=384 \\\n",
    "#  --version_2_with_negative=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow_models import nlp, optimization\n",
    "from keras import metrics, layers, optimizers, losses, Model, models, callbacks\n",
    "from keras.utils import plot_model\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs_folder_bert = \"gs://cloud-tpu-checkpoints/bert/v3/uncased_L-12_H-768_A-12\"\n",
    "tf.io.gfile.listdir(gs_folder_bert)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "database_dir = \"/content/drive/MyDrive/ml\"\n",
    "checkpoint_dir = database_dir+\"/model_save\"\n",
    "bert_dir = gs_folder_bert\n",
    "squad_version = \"v2.0\"\n",
    "squad_dir = database_dir+\"/squad\"\n",
    "max_seq_length = 384\n",
    "train_batch_size = 12\n",
    "train_epochs=5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_dataset = tf.data.TFRecordDataset([os.path.join(\n",
    "    squad_dir, f\"train_{squad_version}.tf_record\")])\n",
    "\n",
    "feature_description = {\n",
    "    'unique_ids': tf.io.FixedLenFeature([], tf.int64, default_value=0),\n",
    "    'input_ids': tf.io.FixedLenFeature(shape=(max_seq_length,), dtype=tf.int64, default_value=[0] * max_seq_length),\n",
    "    'input_mask': tf.io.FixedLenFeature(shape=(max_seq_length,), dtype=tf.int64, default_value=[0] * max_seq_length),\n",
    "    'segment_ids': tf.io.FixedLenFeature(shape=(max_seq_length,), dtype=tf.int64, default_value=[0] * max_seq_length),\n",
    "    'paragraph_mask': tf.io.FixedLenFeature(shape=(max_seq_length,), dtype=tf.int64, default_value=[0] * max_seq_length),\n",
    "    'class_index': tf.io.FixedLenFeature([], tf.int64, default_value=0),\n",
    "    'start_positions': tf.io.FixedLenFeature([], tf.int64, default_value=0),\n",
    "    'end_positions': tf.io.FixedLenFeature([], tf.int64, default_value=0),\n",
    "    'is_impossible': tf.io.FixedLenFeature([], tf.int64, default_value=0)\n",
    "}\n",
    "\n",
    "def decode_fn(record_bytes):\n",
    "    example = tf.io.parse_single_example(\n",
    "        record_bytes,\n",
    "        feature_description\n",
    "    )\n",
    "    # example[\"input_word_ids\"] = example[\"input_ids\"]\n",
    "    # example[\"input_type_ids\"] = example[\"segment_ids\"]\n",
    "\n",
    "    return {\n",
    "        'input_word_ids': example[\"input_ids\"],\n",
    "        'input_type_ids': example[\"segment_ids\"],\n",
    "        'input_mask': example[\"input_mask\"],\n",
    "        'is_impossible': example[\"is_impossible\"]\n",
    "    }, {\n",
    "        \"start_positions\": example[\"start_positions\"],\n",
    "        \"end_positions\": example[\"end_positions\"],\n",
    "    }\n",
    "\n",
    "\n",
    "# for raw_record in raw_dataset.take(10).map(decode_fn):\n",
    "#     print(raw_record)\n",
    "\n",
    "bert_config_file = os.path.join(bert_dir, 'bert_config.json')\n",
    "config_dict = json.loads(tf.io.gfile.GFile(bert_config_file).read())\n",
    "\n",
    "dataset = raw_dataset.shuffle(buffer_size=train_batch_size).map(decode_fn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 总条目131944,可用条目87212\n",
    "train_num = 120000\n",
    "\n",
    "\n",
    "train_ds = dataset.take(train_num).batch(batch_size=train_batch_size)\n",
    "\n",
    "test_ds = dataset.skip(train_num).batch(batch_size=train_batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_optimizer():\n",
    "    train_data_size = train_num\n",
    "    steps_per_epoch = int(train_data_size / train_batch_size)\n",
    "    num_train_steps = steps_per_epoch * train_epochs\n",
    "    warmup_steps = int(0.1 * num_train_steps)\n",
    "    initial_learning_rate = 2e-5\n",
    "\n",
    "    linear_decay = tf.keras.optimizers.schedules.PolynomialDecay(\n",
    "        initial_learning_rate=initial_learning_rate,\n",
    "        end_learning_rate=0,\n",
    "        decay_steps=num_train_steps)\n",
    "\n",
    "    warmup_schedule = optimization.lr_schedule.LinearWarmup(\n",
    "        warmup_learning_rate=0,\n",
    "        after_warmup_lr_sched=linear_decay,\n",
    "        warmup_steps=warmup_steps\n",
    "    )\n",
    "\n",
    "    optimizer = optimizers.Adam(\n",
    "        learning_rate=warmup_schedule)\n",
    "\n",
    "    return optimizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model():\n",
    "    encoder_config = nlp.encoders.EncoderConfig({\n",
    "        'type': 'bert',\n",
    "        'bert': config_dict\n",
    "    })\n",
    "\n",
    "    bert_encoder = nlp.encoders.build_encoder(encoder_config)\n",
    "    #bert_encoder.trainable = False\n",
    "    # output='predictions'\n",
    "    bert_span = nlp.models.BertSpanLabeler(network=bert_encoder)\n",
    "    \n",
    "    checkpoint = tf.train.Checkpoint(encoder=bert_encoder)\n",
    "\n",
    "    checkpoint.read(\n",
    "        os.path.join(bert_dir, 'bert_model.ckpt')\n",
    "    ).assert_consumed()\n",
    "\n",
    "    return bert_span"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_or_restore_model():\n",
    "\n",
    "    if not os.path.exists(checkpoint_dir):\n",
    "        os.makedirs(checkpoint_dir)\n",
    "\n",
    "    checkpoints = [checkpoint_dir + \"/\" +\n",
    "                   name for name in os.listdir(checkpoint_dir)]\n",
    "\n",
    "    if checkpoints:\n",
    "        latest_checkpoint = max(checkpoints, key=os.path.getctime)\n",
    "\n",
    "        print(\"Restoring from\", latest_checkpoint)\n",
    "        \n",
    "        model = models.load_model(latest_checkpoint, compile=False)\n",
    "\n",
    "    else:\n",
    "        print(\"Creating a new model\")\n",
    "        model = build_model()\n",
    "\n",
    "    metrics = [tf.keras.metrics.SparseCategoricalAccuracy('accuracy', dtype=tf.float32)]\n",
    "\n",
    "    loss = [losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "            losses.SparseCategoricalCrossentropy(from_logits=True)]\n",
    "\n",
    "    optimizer = get_optimizer()\n",
    "\n",
    "    model.compile(optimizer=optimizer,\n",
    "                  loss=loss,\n",
    "                  metrics=metrics)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = make_or_restore_model()\n",
    "\n",
    "callbacks = [\n",
    "    # This callback saves a SavedModel every 100 batches.\n",
    "    # We include the training loss in the saved model name.\n",
    "    callbacks.ModelCheckpoint(\n",
    "        filepath=checkpoint_dir + \"/ckpt-{epoch:02d}-{val_loss:.2f}\",\n",
    "        save_best_only=True,\n",
    "        save_freq='epoch',\n",
    "        verbose=1\n",
    "    )\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_model(bert_span, show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(test_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(train_ds,\n",
    "          validation_data=(test_ds),\n",
    "          batch_size=train_batch_size,\n",
    "          epochs=train_epochs,\n",
    "          callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models.saved_model(model, \"./qa_model\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
